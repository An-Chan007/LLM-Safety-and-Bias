# LLM-Safety-and-Bias
A comprehensive framework for evaluating and mitigating bias and safety risks in Large Language Models (LLMs) across healthcare, workplace, and smart city scenarios, featuring advanced analysis, visualization, and reporting capabilities.
